{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-weight:bold\">Jayant Solanki</span>,\n",
    "<span style=\"color:red;font-weight:bold\">Anant Gupta</span>\n",
    "<hr/>\n",
    "## <span style=\"float:left\">Lab 2</span>\n",
    "### <span style=\"float:right\">DATA AGGREGATION , BIG DATA ANALYSIS AND VISUALIZATION</span>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Background: The nytimes api doesn't returns full article body, just the snippet of it each returned\n",
    "# so, I have to write the code to fetch the weburl for each article, then perfrom further requests to those url and \n",
    "#fetch the story body of each article\n",
    "import requests # for performing html request to nytimes API\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import lxml.html as html # for scrapping the content of the article URL of nytimes\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each API keys can provide 10000 response hits\n",
    "topic = \"trade+war\"#topic to be looked for\n",
    "apikey = \"527c18ffc4e648cb936f582a0e264ff1\"\n",
    "fl = \"snippet,web_url\"#selective attributes of json response\n",
    "pageNo = \"0\"#initial page is 0, articles fetched using api are grouped in 10 per page starting 0 and upto page 100\n",
    "dateRange = [\"20180408\", \"20180409\"]\n",
    "# dateRange = [\"20180321\", \"20180322\", \"20180323\", \"20180324\", \"20180325\", \"20180326\", \"20180327\", \"20180328\"]# can be changed t any period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function parse the json response from nytimes and create new dictionary using two attributes only\n",
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    fetch = articles['response']['docs']\n",
    "    for i in range(0,len(fetch)):\n",
    "        dic = {}\n",
    "#         print(fetch[i])\n",
    "        dic['web_url'] = fetch[i]['web_url']\n",
    "        if fetch[i]['snippet'] is not None:\n",
    "            dic['snippet'] = fetch[i]['snippet']\n",
    "#         dic['url'] = i['web_url']\n",
    "        news.append(dic)\n",
    "    return(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function perfrom request to nytimes api using the paramters passed and returns the parseed responsed to the caller function\n",
    "def get_articles(topic, begin_date, end_date, fl, apikey):\n",
    "    all_articles = []#stores all articles for a particular day\n",
    "    page = 0\n",
    "    while(page<100):\n",
    "        sleep(1)\n",
    "#     for page in range(0,100): #NYT limits pages to first 100 pages starting page 0, each page has 10 articles max\n",
    "        try:\n",
    "            \n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\"+topic+\"&begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&fl=\"+fl+\"&page=\"+str(page)+\"&api-key=\"+apikey\n",
    "            print(url)\n",
    "            requestArticles = requests.get(url)\n",
    "            data = requestArticles.json()\n",
    "            if len(data[\"response\"][\"docs\"])>0:\n",
    "                all_articles.append(parse_articles(data))\n",
    "#                 print(data)\n",
    "            else:# checks if further pages have no articles to show, if yes then break the loop and return the fetched articles\n",
    "                print(parse_articles(data))\n",
    "                break\n",
    "        except:\n",
    "            print(\"You called the api way to fast, Dude, trying again\")\n",
    "            print(data)\n",
    "#             page = page - 1\n",
    "            sleep(1)\n",
    "            continue#try again\n",
    "        print(\"Page: \"+str(page))\n",
    "#         break\n",
    "        page=page+1\n",
    "    return(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for Data period: 04-08-2018 - 04-09-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=0&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=1&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 1\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=2&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 2\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=3&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 3\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=4&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 4\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=5&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "Page: 5\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=trade+war&begin_date=20180408&end_date=20180409&fl=snippet,web_url&page=6&api-key=527c18ffc4e648cb936f582a0e264ff1\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0e90c8f08670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatetimeobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateRange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbeginDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetimeobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%m-%d-%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdatetimeobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateRange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mendDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetimeobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%m-%d-%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fetching articles for Data period: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeginDate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" - \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mendDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#caller function\n",
    "processArticles = []\n",
    "for i in range(0,1):\n",
    "    datetimeobject = datetime.strptime(dateRange[i],'%Y%m%d')\n",
    "    beginDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "    datetimeobject = datetime.strptime(dateRange[i+1],'%Y%m%d')\n",
    "    endDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "    print(\"Fetching articles for Data period: \" + beginDate + \" - \"+ endDate)\n",
    "    processArticles = get_articles(topic, dateRange[i], dateRange[i+1],fl, apikey)\n",
    "    if(len(processArticles)>0):\n",
    "#         try:\n",
    "#             dataToWrite = processArticles\n",
    "# #             print(dataToWrite)\n",
    "#         except:\n",
    "#             print(len(processArticles))\n",
    "#             print(processArticles)\n",
    "#             print(processArticles[0])\n",
    "#             break\n",
    "        with open(\"textcorpus/\"+topic+dateRange[i]+\".txt\", 'w') as outfile:#used for storing snippets\n",
    "            for item in processArticles:\n",
    "                for articles in item:\n",
    "#                     print(articles)\n",
    "                    outfile.write(articles[\"snippet\"])\n",
    "                    outfile.write(\"\\n\")\n",
    "        with open(\"textcorpus/\"+topic+dateRange[i]+\"-full.txt\", 'w') as outfile:#used for storing full articles\n",
    "            for item in processArticles:\n",
    "                for articles in item:\n",
    "                    fullpage = requests.get(articles[\"web_url\"])\n",
    "                    htmlbody = html.fromstring(requests.get(articles[\"web_url\"]).content)\n",
    "                    output = \"\".join(htmlbody.xpath('//p[contains(@class,\"story-body-text\")]//text()'))#scrapper\n",
    "#                     print(output+\"\\n\\n\")\n",
    "#                     output = output.decode('utf8').encode('latin1').decode('utf8')\n",
    "                    output = str(output.encode(\"ascii\", \"ignore\"))#since the body has lots of escape characters, I have to convert\n",
    "    # utf-8 response into ascii using ignore flag to bypass those escape characters\n",
    "                    outfile.write(output[2:-1])\n",
    "                    outfile.write(\"\\n\")\n",
    "    else:\n",
    "        print(\"Insufficient data for date: \"+beginDate+\" to save\")\n",
    "#     break\n",
    "# print(processArticles[0][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.8.3.jar -input /user/jayant/input/ -output /user/jayant/output_new2 -mapper /home/jayant/wordcount_mapper.py -reducer /home/jayant/wordcount_reducer.py -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red;font-weight:bold\">Anant Gupta</span>, \n",
    "<span style=\"color:red;font-weight:bold\">Jayant Solanki</span>\n",
    "<hr/>\n",
    "## <span style=\"float:left\">Finding trending words from a news topic on twitter.</span>\n",
    "### <span style=\"float:right\">Using twitter API to fetch tweets for a particular search topic</span>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we will collect the tweets in batches for each dates using the twitter api. There are limitations to collect\n",
    "# the tweets using the twitter api. So in order to overcome with this limitation we have kept a waiting period before\n",
    "# starting to fetch the tweets again\n",
    "# Below are the modules which we need to import for this twitter data collection script\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from datetime import date\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the main class which we will use to perform the data collection using the twitter api.\n",
    "# In the init method we will give the api keys and intialize the api connection.\n",
    "\n",
    "class TwitterDataCollecion(object):\n",
    "    \n",
    "    def __init__(self,query,tweetscount):\n",
    "        \n",
    "        #Initialize the twitter api connection\n",
    "        consumer_key = 'M3uDdeL5NzbjfdBGVtL5r9PZu'\n",
    "        consumer_secret = 'yF3yQnM05yuZgVSDKsaMWePs3M93f7XQh6PQHOQazyytvlMzSM'\n",
    "        access_token = '417187736-o3vz5smd7Y50QIfPxVpWdKTTA645OwRRDJhfRZBU'\n",
    "        access_secret = 'x7IbwZMcIgl29ee5uYbaHfOf5VUQh2e0rkbNFasTjk5Mq'\n",
    "         \n",
    "        auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_secret)\n",
    "        self.api = tweepy.API(auth)\n",
    "        self.query = query\n",
    "        self.tcount = tweetscount\n",
    "\n",
    "\n",
    "    def search_tweet(self):\n",
    "        ''' Function that takes in a search string 'query', the maximum\n",
    "            number of tweets 'self.tcount'. It returns a list of \n",
    "            tweepy.models.Status objects. '''\n",
    "     \n",
    "        searched_tweets = []\n",
    "        while len(searched_tweets) < self.tcount:\n",
    "            remaining_tweets = self.tcount - len(searched_tweets)\n",
    "            try:\n",
    "                new_tweets = self.api.search(q=self.query, count=remaining_tweets)\n",
    "                print('found',len(new_tweets),'tweets')\n",
    "                if not new_tweets:\n",
    "                    print('no tweets found')\n",
    "                    break\n",
    "                searched_tweets.extend(new_tweets)\n",
    "            except tweepy.TweepError:\n",
    "                print('exception raised, waiting 1 minutes')\n",
    "                time.sleep(1*60)\n",
    "                continue\n",
    "                #Continue the tweets collection after sleeping for 1 mins\n",
    "        return searched_tweets\n",
    "    \n",
    "    \n",
    "    def clean_text_and_tokenize(self,line):        \n",
    "        tokens = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize(line)\n",
    "        \n",
    "        return tokens\n",
    "       \n",
    "    def clean_tweet(self,tweet):\n",
    "        return \" \".join(self.clean_text_and_tokenize(tweet))\n",
    "    \n",
    "    def write_tweets(self,tweets, filename,encoding=\"utf-8\"):\n",
    "        ''' Function that appends the cleansed tweets to a file. '''\n",
    "     \n",
    "        with open(filename, 'a') as f:\n",
    "            for tweet in tweets:\n",
    "                # t = self.clean_tweet(tweet._json[\"text\"])\n",
    "                t = tweet._json[\"text\"]\n",
    "                t = str(t.encode(\"ascii\", \"ignore\"))\n",
    "                # index = t.find(':')\n",
    "                f.write(t[t.find(':')+2:-1])\n",
    "                f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweets for trade+war\n",
      "found 100 tweets\n",
      "found 100 tweets\n",
      "found 100 tweets\n",
      "found 100 tweets\n",
      "found 100 tweets\n",
      "Twitter Data is ready for further analysis\n"
     ]
    }
   ],
   "source": [
    "# This is the main driver function where we will give the topic and the count of the tweets we need.\n",
    "# Once the data is ready it is saved in a text file with timestamp in it name.\n",
    "# Here we will try with a small dataset for the trade war topic.\n",
    "# Note: The data is collected in batches of 100 (restriction imposed by twitter api)\n",
    "\n",
    "def main():\n",
    "    query = \"trade+war\"\n",
    "    print(\"Collecting tweets for \"+ query)\n",
    "    today = date.today().strftime(\"%d%m%y\")\n",
    "    maxtweets = 500\n",
    "    twitterclass = TwitterDataCollecion(query,maxtweets)\n",
    "    tweets=twitterclass.search_tweet()\n",
    "    twitterclass.write_tweets(tweets, \"textcorpus/tweetsdata\"+\"-\"+query+\"-\"+today+\".txt\")\n",
    "    print(\"Twitter Data is ready for further analysis\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "## <span style=\"float:left\">After collecting the data from both the sources (NY Times and Twitter) we'll perform the data analysis part with the following mapper and reducer code.</span>\n",
    "### <span style=\"float:right\">Below is the mapper and reducer script for getting the word frequency</span>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordcount_mapper.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#the above just indicates to use python to intepret this file\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#This mapper code will input a line of text and output <word, 1>\n",
    "# \n",
    "# ---------------------------------------------------------------\n",
    "import re\n",
    "import string\n",
    "import sys             #a python module with system functions for this OS\n",
    "from nltk.corpus import stopwords\n",
    "stop_word_list = stopwords.words('english')\n",
    "#  to quickly test if a word is not a stop word, use a set:\n",
    "stop_word_set = set(stop_word_list)\n",
    "# ------------------------------------------------------------\n",
    "#  this 'for loop' will set 'line' to an input line from system \n",
    "#    standard input file\n",
    "# ------------------------------------------------------------\n",
    "for line in sys.stdin:  \n",
    "\n",
    "#-----------------------------------\n",
    "#sys.stdin call 'sys' to read a line from standard input, \n",
    "# note that 'line' is a string object, ie variable, and it has methods that you can apply to it,\n",
    "# as in the next line\n",
    "# ---------------------------------\n",
    "    line = line.strip()  #strip is a method, ie function, associated\n",
    "                         #  with string variable, it will strip \n",
    "                         #   the carriage return (by default)\n",
    "    keys = line.split()  #split line at blanks (by default), \n",
    "                         #   and return a list of keys\n",
    "    for key in keys:     #a for loop through the list of keys\n",
    "        value = 1\n",
    "        key=key.strip('\"')\n",
    "        key=key.strip('.')\n",
    "        key=key.strip('“')\n",
    "        key=key.strip('$')\n",
    "        key=key.strip('?')\n",
    "        key=key.strip(',')\n",
    "        key=key.strip(')')\n",
    "        key=key.strip('(')\n",
    "        key=key.strip(' ')\n",
    "        key   = re.sub(r'\\$\\w*', '', key)\n",
    "        key   = re.sub(r'http?:.*$', '', key)\n",
    "        key   = re.sub(r'https?:.*$', '', key)\n",
    "        key   = re.sub(r'pic?.*\\/\\w*', '', key)\n",
    "        key   = re.sub(r'[' + string.punctuation + ']+', ' ', key)  # Remove puncutations like 's\n",
    "        key=str.replace(key,'\\'s','')\n",
    "        key=str.replace(key,'\\\\','')\n",
    "        key=str.replace(key,'s\\'','')\n",
    "        if(key[:1].isdigit() or key[:1]=='$' or key[:1]=='&' or len(key)<=2 or key[:1]=='\"' or key[:1]=='\\'' or not key.isalpha()):\n",
    "            continue\n",
    "        if key.lower() not in stop_word_set:\n",
    "            print('{0}\\t{1}'.format(key, value) ) #the {} is replaced by 0th,1st items in format list\n",
    "                            #also, note that the Hadoop default is 'tab' separates key from the value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word,count\n",
      "None,0\n"
     ]
    }
   ],
   "source": [
    "#wordcount_reducer.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------\n",
    "#This reducer code will input a line of text and \n",
    "#    output <word, total-count>\n",
    "# ---------------------------------------------------------------\n",
    "import sys\n",
    "last_key,this_key      = None,None              #initialize these variables\n",
    "running_total = 0\n",
    "\n",
    "# -----------------------------------\n",
    "# Loop thru file\n",
    "#  --------------------------------\n",
    "print( \"{0},{1}\".format(\"word\",\"count\")) \n",
    "for input_line in sys.stdin:\n",
    "    input_line = input_line.strip()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Get Next Word    # --------------------------------\n",
    "    vals = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "    if(len(vals)<2):\n",
    "        continue\n",
    "    this_key, value = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "                          #the split command returns a list of strings, in this case into 2 variables\n",
    "    value = int(value)           #int() will convert a string to integer (this program does no error checking)\n",
    " \n",
    "    # ---------------------------------\n",
    "    # Key Check part\n",
    "    #    if this current key is same \n",
    "    #          as the last one Consolidate\n",
    "    #    otherwise  Emit\n",
    "    # ---------------------------------\n",
    "    if last_key == this_key:     #check if key has changed ('==' is                                   #      logical equalilty check\n",
    "        running_total += value   # add value to running total\n",
    "\n",
    "    else:\n",
    "        if last_key:             #if this key that was just read in\n",
    "                                 #   is different, and the previous \n",
    "                                 #   (ie last) key is not empy,\n",
    "                                 #   then output \n",
    "                                 #   the previous <key running-count>\n",
    "            print( \"{0},{1}\".format(last_key, running_total) )\n",
    "                                 # hadoop expects tab(ie '\\t') \n",
    "                                 #    separation\n",
    "        running_total = value    #reset values\n",
    "        last_key = this_key\n",
    "\n",
    "if last_key == this_key:\n",
    "    print( \"{0},{1}\".format(last_key, running_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "## <span style=\"float:left\">Next we'll perform the data analysis part with the following mapper and reducer code to find the co-occurence words and its corresponding frequency </span>\n",
    "<hr/>\n",
    "### Please refer to the hadoop.sh and hadoop-co-occurrence.sh bash scripts for running the map-reduce scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#co_occurence_mapper.py\n",
    "\n",
    "#!/usr/bin/env python  \n",
    "# -*- coding: utf-8 -*- \n",
    "#the above just indicates to use python to intepret this file\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#This mapper code will input a line of text and output <(word,co_occurenceword), 1>\n",
    "# \n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import sys             #a python module with system functions for this OS\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_word_list = stopwords.words('english')\n",
    "#  to quickly test if a word is not a stop word, use a set:\n",
    "stop_word_set = set(stop_word_list)\n",
    "# ------------------------------------------------------------\n",
    "#  this 'for loop' will set 'line' to an input line from system \n",
    "#    standard input file\n",
    "# ------------------------------------------------------------\n",
    "for line in sys.stdin:  \n",
    "\n",
    "#-----------------------------------\n",
    "#sys.stdin call 'sys' to read a line from standard input, \n",
    "# note that 'line' is a string object, ie variable, and it has methods that you can apply to it,\n",
    "# as in the next line\n",
    "# ---------------------------------\n",
    "    line = line.strip()  #strip is a method, ie function, associated\n",
    "                         #  with string variable, it will strip \n",
    "                         #   the carriage return (by default)\n",
    "    words = line.split()  #split line at blanks (by default), \n",
    "                         #   and return a list of words\n",
    "    value = 1\n",
    "\n",
    "    final_words = []\n",
    "    for word in words:     #a for loop through the list of words\n",
    "        word=word.strip('\"')\n",
    "        word=word.strip('.')\n",
    "        word=word.strip('“')\n",
    "        word=word.strip('$')\n",
    "        word=word.strip('?')\n",
    "        word=word.strip(',')\n",
    "        word=word.strip(')')\n",
    "        word=word.strip('(')\n",
    "        word=word.strip(' ')\n",
    "        word   = re.sub(r'\\$\\w*', '', word)\n",
    "        word   = re.sub(r'http?:.*$', '', word)\n",
    "        word   = re.sub(r'https?:.*$', '', word)\n",
    "        word   = re.sub(r'pic?.*\\/\\w*', '', word)\n",
    "        word   = re.sub(r'[' + string.punctuation + ']+', ' ', word)\n",
    "        word=str.replace(word,'\\'s','')\n",
    "        word=str.replace(word,'\\\\','')\n",
    "        word=str.replace(word,'s\\'','')\n",
    "        if(word[:1].isdigit() or word[:1]=='$' or word[:1]=='&' or len(word)<=2 or word[:1]=='\"' or word[:1]=='\\'' or not word.isalpha()):\n",
    "            continue\n",
    "        if word.lower() not in stop_word_set:\n",
    "            final_words.append(word.lower())\n",
    "\n",
    "    if len(final_words):\n",
    "        count = len(final_words)\n",
    "        for i in range(count-1):\n",
    "            print('{0}\\t{1}'.format(final_words[i]+'-'+final_words[i+1], value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word,count\n",
      "None,0\n"
     ]
    }
   ],
   "source": [
    "#co-occurence_reducer.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------\n",
    "#This reducer code will input a line of text and \n",
    "#    output <(word-co-occurence_word), total-count>\n",
    "# ---------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "last_key      = None              #initialize these variables\n",
    "running_total = 0\n",
    "\n",
    "# -----------------------------------\n",
    "# Loop thru file\n",
    "#  --------------------------------\n",
    "print( \"{0},{1}\".format(\"word\",\"count\")) \n",
    "for input_line in sys.stdin:\n",
    "    input_line = input_line.strip()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Get Next Word    # --------------------------------\n",
    "    vals = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "    if(len(vals)<2):\n",
    "        continue\n",
    "    this_key, value = input_line.split(\"\\t\", 1)  #the Hadoop default is tab separates key value\n",
    "                          #the split command returns a list of strings, in this case into 2 variables\n",
    "    value = int(value)           #int() will convert a string to integer (this program does no error checking)\n",
    " \n",
    "    # ---------------------------------\n",
    "    # Key Check part\n",
    "    #    if this current key is same \n",
    "    #          as the last one Consolidate\n",
    "    #    otherwise  Emit\n",
    "    # ---------------------------------\n",
    "    if last_key == this_key:     #check if key has changed ('==' is                                   #      logical equalilty check\n",
    "        running_total += value   # add value to running total\n",
    "\n",
    "    else:\n",
    "        if last_key:             #if this key that was just read in\n",
    "                                 #   is different, and the previous \n",
    "                                 #   (ie last) key is not empy,\n",
    "                                 #   then output \n",
    "                                 #   the previous <key running-count>\n",
    "            print( \"{0},{1}\".format(last_key, running_total) )\n",
    "                                 # hadoop expects tab(ie '\\t') \n",
    "                                 #    separation\n",
    "        running_total = value    #reset values\n",
    "        last_key = this_key\n",
    "\n",
    "if last_key == this_key:\n",
    "    print( \"{0},{1}\".format(last_key, running_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
